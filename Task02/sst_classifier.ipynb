{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of sst_classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6BdTni9Q2eA",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment Classification with a Deep Learning Model\n",
        "\n",
        "This notebook introduces a machine learning task from the field of natural language processing (machine learning focused on the processing of spoken and written text).\n",
        "\n",
        "## Sentiment Analysis\n",
        "\n",
        "The modelled task is a classification task called sentiment analysis. \n",
        "Text snippets are classified according to their positive or negative sentiment that is expressed in them. \n",
        "This can be modelled as 3-class problem (negative, neutral, positive), or as a degree of sentiment on a 5-class or 10-class scale. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Acknowledgement\n",
        "\n",
        "The notebook is based on https://www.manning.com/books/real-world-natural-language-processing, an upcoming book focused on NLP.\n",
        "\n",
        "The ML frameworks used are:\n",
        "\n",
        "* pytorch\n",
        "* allennlp\n",
        "* spacy\n",
        "\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/erikgraf/deepLearning/blob/master/Deep_Learning_Sentiment_classifier.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYJIROXeQ2eK",
        "colab_type": "text"
      },
      "source": [
        "## Installing Dependencies\n",
        "\n",
        "The cell below installs the main dependencies and clones some a repository that forms the basis of the implementation. \n",
        "\n",
        "Executing it with `CTRL + Enter` (`STRG +Enter` on a german keyboard) could take a couple of minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2WUYWOVTcajS",
        "outputId": "9ae1c087-e371-4f69-9042-7ecae6305908",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install allennlp"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting allennlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/bb/041115d8bad1447080e5d1e30097c95e4b66e36074277afce8620a61cee3/allennlp-0.9.0-py3-none-any.whl (7.6MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6MB 8.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.3.0)\n",
            "Collecting parsimonious>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/fc/067a3f89869a41009e1a7cdfb14725f8ddd246f30f63c645e8ef8a1c56f4/parsimonious-0.8.1.tar.gz (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.1.2)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.21.0)\n",
            "Collecting pytorch-pretrained-bert>=0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 59.1MB/s \n",
            "\u001b[?25hCollecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 62.0MB/s \n",
            "\u001b[?25hCollecting tensorboardX>=1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/f1/5843425495765c8c2dd0784a851a93ef204d314fc87bcc2bbb9f662a3ad1/tensorboardX-2.0-py2.py3-none-any.whl (195kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 60.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.22.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.17.5)\n",
            "Requirement already satisfied: spacy<2.2,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.1.9)\n",
            "Collecting overrides\n",
            "  Downloading https://files.pythonhosted.org/packages/72/dd/ac49f9c69540d7e09210415801a05d0a54d4d0ca8401503c46847dacd3a0/overrides-2.8.0.tar.gz\n",
            "Requirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.5)\n",
            "Collecting jsonpickle\n",
            "  Downloading https://files.pythonhosted.org/packages/07/07/c157520a3ebd166c8c24c6ae0ecae7c3968eb4653ff0e5af369bb82f004d/jsonpickle-1.2-py2.py3-none-any.whl\n",
            "Collecting numpydoc>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/b0/70/4d8c3f9f6783a57ac9cc7a076e5610c0cc4a96af543cafc9247ac307fbfe/numpydoc-0.9.2.tar.gz\n",
            "Collecting flask-cors>=3.0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/78/38/e68b11daa5d613e3a91e4bf3da76c94ac9ee0d9cd515af9c1ab80d36f709/Flask_Cors-3.0.8-py2.py3-none-any.whl\n",
            "Collecting pytorch-transformers==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/89/ad0d6bb932d0a51793eaabcf1617a36ff530dc9ab9e38f765a35dc293306/pytorch_transformers-1.1.0-py3-none-any.whl (158kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 57.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.6.4)\n",
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ca/2d9a5030eaf1bcd925dab392762b9709a7ad4bd486a90599d93cd79cb188/ftfy-5.6.tar.gz (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 10.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: gevent>=1.3.6 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2018.9)\n",
            "Collecting conllu==1.3.1\n",
            "  Downloading https://files.pythonhosted.org/packages/ae/54/b0ae1199f3d01666821b028cd967f7c0ac527ab162af433d3da69242cea2/conllu-1.3.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.10.47)\n",
            "Collecting word2number>=1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n",
            "Collecting jsonnet>=0.10.0; sys_platform != \"win32\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/a6/e69e38f1f259fcf8532d8bd2c4bc88764f42d7b35a41423a7f4b035cc5ce/jsonnet-0.14.0.tar.gz (253kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 29.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.3.1)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp) (4.28.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.1)\n",
            "Collecting flaky\n",
            "  Downloading https://files.pythonhosted.org/packages/fe/12/0f169abf1aa07c7edef4855cca53703d2e6b7ecbded7829588ac7e7e3424/flaky-3.6.1-py2.py3-none-any.whl\n",
            "Collecting responses>=0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/3e/0c/940781dd49710f4b1f0650c450c9fd8491db0e1bffd99ebc36355607f96d/responses-0.10.9-py2.py3-none-any.whl\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.8.0)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.5.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from parsimonious>=0.8.0->allennlp) (1.12.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.6.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.4.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (1.1.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (1.24.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert>=0.6.0->allennlp) (2019.12.20)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp) (3.10.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->allennlp) (0.14.1)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.2.4)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.9.6)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (2.0.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (1.0.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (2.0.3)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (7.0.8)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (1.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.6.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (0.16.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (7.0)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (2.10.3)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (1.1.0)\n",
            "Requirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (1.8.5)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 53.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (42.0.2)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (8.0.2)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (0.7.1)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.3.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.8.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (19.3.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->allennlp) (0.1.8)\n",
            "Requirement already satisfied: greenlet>=0.4.14; platform_python_implementation == \"CPython\" in /usr/local/lib/python3.6/dist-packages (from gevent>=1.3.6->allennlp) (0.4.15)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.47 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (1.13.47)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.9.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->flask>=1.0.2->allennlp) (1.1.1)\n",
            "Requirement already satisfied: docutils>=0.11 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (0.15.2)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.8.0)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (20.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.1.3)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (0.7.12)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.1.2)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.2.0)\n",
            "Building wheels for collected packages: parsimonious, overrides, numpydoc, ftfy, word2number, jsonnet\n",
            "  Building wheel for parsimonious (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parsimonious: filename=parsimonious-0.8.1-cp36-none-any.whl size=42709 sha256=95bbbfc8b4feac6f3acd28984fe24cb93e0ada7c4c715b42b3d66970623867d8\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/8d/e7/a0e74217da5caeb3c1c7689639b6d28ddbf9985b840bc96a9a\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-2.8.0-cp36-none-any.whl size=5608 sha256=cd13f3b74d602c9babf1848dd8e13aaa449e35e6e0daa00363b45fb4ec0f2f97\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/f1/ba/eaf6cd7d284d2f257dc71436ce72d25fd3be5a5813a37794ab\n",
            "  Building wheel for numpydoc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for numpydoc: filename=numpydoc-0.9.2-cp36-none-any.whl size=31894 sha256=47a39a1fdd707394f977da40bfc6c3e1f1302b8229d21cd4b6a83ac0379e8241\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/f3/52/25c8e1f40637661d27feebc61dae16b84c7cdd93b8bc3d7486\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.6-cp36-none-any.whl size=44553 sha256=c7ffb7874c6ab31473a7e54b73bf34993c7010348b871e5a3501f05e73833831\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/34/ce/cbb38d71543c408de56f3c5e26ce8ba495a0fa5a28eaaf1046\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-cp36-none-any.whl size=5588 sha256=1e10b0ca03e847c25fcedaf05aa7f4fe2aaf43a423d06096896ef138eb8e012d\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.14.0-cp36-cp36m-linux_x86_64.whl size=3320369 sha256=1d09796c4b239014a4d9cdd99c38a2ea701aa9278f9c8af9f27a735a2329e661\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/b7/83/985f0f758fbb34f14989a0fab86d18890d1cc5ae12f26967bc\n",
            "Successfully built parsimonious overrides numpydoc ftfy word2number jsonnet\n",
            "Installing collected packages: parsimonious, pytorch-pretrained-bert, unidecode, tensorboardX, overrides, jsonpickle, numpydoc, flask-cors, sentencepiece, pytorch-transformers, ftfy, conllu, word2number, jsonnet, flaky, responses, allennlp\n",
            "Successfully installed allennlp-0.9.0 conllu-1.3.1 flaky-3.6.1 flask-cors-3.0.8 ftfy-5.6 jsonnet-0.14.0 jsonpickle-1.2 numpydoc-0.9.2 overrides-2.8.0 parsimonious-0.8.1 pytorch-pretrained-bert-0.6.2 pytorch-transformers-1.1.0 responses-0.10.9 sentencepiece-0.1.85 tensorboardX-2.0 unidecode-1.1.1 word2number-1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3w5X2xXQ2ek",
        "colab_type": "code",
        "outputId": "98ba07c0-e09c-4a99-9e61-0c54e80397e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "!git clone https://github.com/mhagiwara/realworldnlp.git\n",
        "%cd realworldnlp"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'realworldnlp'...\n",
            "remote: Enumerating objects: 180, done.\u001b[K\n",
            "remote: Counting objects: 100% (180/180), done.\u001b[K\n",
            "remote: Compressing objects: 100% (127/127), done.\u001b[K\n",
            "remote: Total 468 (delta 111), reused 107 (delta 50), pack-reused 288\u001b[K\n",
            "Receiving objects: 100% (468/468), 4.70 MiB | 6.79 MiB/s, done.\n",
            "Resolving deltas: 100% (240/240), done.\n",
            "/content/realworldnlp\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEykcLBeQ2eq",
        "colab_type": "text"
      },
      "source": [
        "## Imports\n",
        "\n",
        "Execute the cell below to load all required modules. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FM5pBpj7cajc",
        "colab": {}
      },
      "source": [
        "from typing import Dict\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from allennlp.data.dataset_readers.stanford_sentiment_tree_bank import \\\n",
        "    StanfordSentimentTreeBankDatasetReader\n",
        "from allennlp.data.iterators import BucketIterator\n",
        "from allennlp.data.vocabulary import Vocabulary\n",
        "from allennlp.models import Model\n",
        "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
        "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
        "from allennlp.modules.token_embedders import Embedding\n",
        "from allennlp.nn.util import get_text_field_mask\n",
        "from allennlp.training.metrics import CategoricalAccuracy, F1Measure\n",
        "from allennlp.training.trainer import Trainer\n",
        "\n",
        "from realworldnlp.predictors import SentenceClassifierPredictor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZnvL_OGQ2ew",
        "colab_type": "text"
      },
      "source": [
        "## Hyperparameters\n",
        "\n",
        "The cell below sets the hyperparameters.\n",
        "\n",
        "* EMBEDDING_DIM: This is the dimensionality of the word embeddings (numeric representations of words such as word2vec or glove (https://nlp.stanford.edu/projects/glove/))\n",
        "* HIDDEN_DIM: This is the dimensionality of the LSTM (Long Short Term Memory) Deep Learning network. \n",
        "\n",
        "A value of 128 is pretty standard for the embeddings and hidden_dim.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jecKS_nhcajq",
        "colab": {}
      },
      "source": [
        "EMBEDDING_DIM = 128\n",
        "HIDDEN_DIM = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxP_1-rPQ2e7",
        "colab_type": "text"
      },
      "source": [
        "## Training Data Set\n",
        "\n",
        "For training we will use the Stanford Sentiment Treebank data set.\n",
        "A data set for training sentiment analysis models. It is annotated both on the sentence and the word level with regard to the sentiment. \n",
        "\n",
        "When loading the data set we can configure the granularity to `'5-class'` or `'3-class'`.\n",
        "\n",
        "`'3-class'` represents classification on the level of `negative`, `neutral`, `positive` encoded as `0`, `1`, `2` (positive). `'5-class'` on a level from `0` to `4`.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b-yyKUYxcaj1",
        "colab": {}
      },
      "source": [
        "reader = StanfordSentimentTreeBankDatasetReader(granularity='5-class')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N4PBhQZAcaj6",
        "outputId": "01c6c6d7-2cce-404b-9fb9-6eab648e8418",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "train_dataset = reader.read('https://s3.amazonaws.com/realworldnlpbook/data/stanfordSentimentTreebank/trees/train.txt')\n",
        "dev_dataset = reader.read('https://s3.amazonaws.com/realworldnlpbook/data/stanfordSentimentTreebank/trees/dev.txt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]\n",
            "  0%|          | 0/2160058 [00:00<?, ?B/s]\u001b[A\n",
            "  2%|▏         | 52224/2160058 [00:00<00:06, 312061.77B/s]\u001b[A\n",
            " 11%|█▏        | 243712/2160058 [00:00<00:04, 398558.24B/s]\u001b[A\n",
            " 43%|████▎     | 922624/2160058 [00:00<00:02, 546061.18B/s]\u001b[A\n",
            "8544it [00:02, 3142.40it/s]\n",
            "0it [00:00, ?it/s]\n",
            "  0%|          | 0/280825 [00:00<?, ?B/s]\u001b[A\n",
            " 19%|█▊        | 52224/280825 [00:00<00:00, 323282.56B/s]\u001b[A\n",
            " 93%|█████████▎| 261120/280825 [00:00<00:00, 415656.98B/s]\u001b[A\n",
            "1101it [00:01, 894.99it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olIoYtPQQ2fI",
        "colab_type": "text"
      },
      "source": [
        "## Model Implementation in AllenNLP\n",
        "\n",
        "Execute the cell below to load the model classification.\n",
        "\n",
        "Depending on the class level chosen (3 vs 5) change the positive label in the init method to ('2' or '4').\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5V2kzVAlcajw",
        "colab": {}
      },
      "source": [
        "# Model in AllenNLP represents a model that is trained.\n",
        "@Model.register(\"lstm_classifier\")\n",
        "class LstmClassifier(Model):\n",
        "    def __init__(self,\n",
        "                 word_embeddings: TextFieldEmbedder,\n",
        "                 encoder: Seq2VecEncoder,\n",
        "                 vocab: Vocabulary,\n",
        "                 positive_label: str = '4') -> None:\n",
        "        super().__init__(vocab)\n",
        "        # We need the embeddings to convert word IDs to their vector representations\n",
        "        self.word_embeddings = word_embeddings\n",
        "\n",
        "        self.encoder = encoder\n",
        "\n",
        "        # After converting a sequence of vectors to a single vector, we feed it into\n",
        "        # a fully-connected linear layer to reduce the dimension to the total number of labels.\n",
        "        self.linear = torch.nn.Linear(in_features=encoder.get_output_dim(),\n",
        "                                      out_features=vocab.get_vocab_size('labels'))\n",
        "\n",
        "        # Monitor the metrics - we use accuracy, as well as prec, rec, f1 for 4 (very positive)\n",
        "        positive_index = vocab.get_token_index(positive_label, namespace='labels')\n",
        "        self.accuracy = CategoricalAccuracy()\n",
        "        self.f1_measure = F1Measure(positive_index)\n",
        "\n",
        "        # We use the cross entropy loss because this is a classification task.\n",
        "        # Note that PyTorch's CrossEntropyLoss combines softmax and log likelihood loss,\n",
        "        # which makes it unnecessary to add a separate softmax layer.\n",
        "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # Instances are fed to forward after batching.\n",
        "    # Fields are passed through arguments with the same name.\n",
        "    def forward(self,\n",
        "                tokens: Dict[str, torch.Tensor],\n",
        "                label: torch.Tensor = None) -> torch.Tensor:\n",
        "        # In deep NLP, when sequences of tensors in different lengths are batched together,\n",
        "        # shorter sequences get padded with zeros to make them equal length.\n",
        "        # Masking is the process to ignore extra zeros added by padding\n",
        "        mask = get_text_field_mask(tokens)\n",
        "\n",
        "        # Forward pass\n",
        "        embeddings = self.word_embeddings(tokens)\n",
        "        encoder_out = self.encoder(embeddings, mask)\n",
        "        logits = self.linear(encoder_out)\n",
        "\n",
        "        # In AllenNLP, the output of forward() is a dictionary.\n",
        "        # Your output dictionary must contain a \"loss\" key for your model to be trained.\n",
        "        output = {\"logits\": logits}\n",
        "        if label is not None:\n",
        "            self.accuracy(logits, label)\n",
        "            self.f1_measure(logits, label)\n",
        "            output[\"loss\"] = self.loss_function(logits, label)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
        "        precision, recall, f1_measure = self.f1_measure.get_metric(reset)\n",
        "        return {'accuracy': self.accuracy.get_metric(reset),\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1_measure': f1_measure}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9ZyejoSQ2fR",
        "colab_type": "text"
      },
      "source": [
        "## Transform Text into Numeric Representation\n",
        "\n",
        "The following cells are responsible for the transformation of text in string form into numeric representations that are suitable as learning input for the neural network.\n",
        "\n",
        "1. Extract vocabulary of unique terms from the text\n",
        "2. Create embeddings for the terms\n",
        "3. Define transformation (encoding) for a sequence of text (i.e. a sentence)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OuchQwj5cakA",
        "outputId": "d047dc04-e0f0-47e1-a279-e83ffdfe8247",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# You can optionally specify the minimum count of tokens/labels.\n",
        "# `min_count={'tokens':3}` here means that any tokens that appear less than three times\n",
        "# will be ignored and not included in the vocabulary.\n",
        "vocab = Vocabulary.from_instances(train_dataset + dev_dataset,\n",
        "                                  min_count={'tokens': 3})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 9645/9645 [00:00<00:00, 85539.22it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kDx14NvHcakC",
        "colab": {}
      },
      "source": [
        "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
        "                            embedding_dim=EMBEDDING_DIM)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VKXSf0wucakG",
        "colab": {}
      },
      "source": [
        "# BasicTextFieldEmbedder takes a dict - we need an embedding just for tokens,\n",
        "# not for labels, which are used as-is as the \"answer\" of the sentence classification\n",
        "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "93pvAweOcakM",
        "colab": {}
      },
      "source": [
        "# Seq2VecEncoder is a neural network abstraction that takes a sequence of something\n",
        "# (usually a sequence of embedded word vectors), processes it, and returns a single\n",
        "# vector. Oftentimes this is an RNN-based architecture (e.g., LSTM or GRU), but\n",
        "# AllenNLP also supports CNNs and other simple architectures (for example,\n",
        "# just averaging over the input vectors).\n",
        "encoder = PytorchSeq2VecWrapper(\n",
        "    torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpvU_tesQ2fr",
        "colab_type": "text"
      },
      "source": [
        "## Configure Model for Training\n",
        "\n",
        "The following four cells configure the model for training.\n",
        "\n",
        "1. The LstmClassifier class takes the word_embeddings, the define sequence encoder and the vocabulary as input configuration. \n",
        "\n",
        "2. The BucketIterator is a helper class for iterating over the full training set and randomly selects batches of instances for the training. \n",
        "\n",
        "3. optimizer specifies the learning rate for Adam (a mathmatical optimisation function that will guide the weight adaptations of our model).\n",
        "\n",
        "4. trainer holds our instatiation of the model, and defines the number of epochs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZuuP66iccakR",
        "colab": {}
      },
      "source": [
        "model = LstmClassifier(word_embeddings, encoder, vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ip0BO9QecakY",
        "colab": {}
      },
      "source": [
        "iterator = BucketIterator(batch_size=32, sorting_keys=[(\"tokens\", \"num_tokens\")])\n",
        "iterator.index_with(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ccuqvd6rcakg",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Uu_dBwd1cakk",
        "colab": {}
      },
      "source": [
        "trainer = Trainer(model=model,\n",
        "                  optimizer=optimizer,\n",
        "                  iterator=iterator,\n",
        "                  train_dataset=train_dataset,\n",
        "                  validation_dataset=dev_dataset,\n",
        "                  patience=40,\n",
        "                  num_epochs=40)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzdyllcfQ2gC",
        "colab_type": "text"
      },
      "source": [
        "## Train\n",
        "\n",
        "Execute the cell below to train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rM5kNKbXQ2gG",
        "colab_type": "code",
        "outputId": "b93e80c0-0ac6-4ee0-8199-a851ad383c3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.2724, precision: 0.0000, recall: 0.0000, f1_measure: 0.0000, loss: 1.5788 ||: 100%|██████████| 267/267 [00:10<00:00, 26.12it/s]\n",
            "accuracy: 0.2534, precision: 0.0000, recall: 0.0000, f1_measure: 0.0000, loss: 1.5785 ||: 100%|██████████| 35/35 [00:00<00:00, 88.76it/s]\n",
            "accuracy: 0.2721, precision: 0.0000, recall: 0.0000, f1_measure: 0.0000, loss: 1.5658 ||: 100%|██████████| 267/267 [00:09<00:00, 26.72it/s]\n",
            "accuracy: 0.2525, precision: 0.0000, recall: 0.0000, f1_measure: 0.0000, loss: 1.5737 ||: 100%|██████████| 35/35 [00:00<00:00, 150.24it/s]\n",
            "accuracy: 0.2773, precision: 0.0000, recall: 0.0000, f1_measure: 0.0000, loss: 1.5571 ||: 100%|██████████| 267/267 [00:09<00:00, 27.54it/s]\n",
            "accuracy: 0.2652, precision: 0.0000, recall: 0.0000, f1_measure: 0.0000, loss: 1.5698 ||: 100%|██████████| 35/35 [00:00<00:00, 162.98it/s]\n",
            "accuracy: 0.2887, precision: 0.5714, recall: 0.0124, f1_measure: 0.0243, loss: 1.5242 ||: 100%|██████████| 267/267 [00:09<00:00, 27.39it/s]\n",
            "accuracy: 0.3052, precision: 0.2000, recall: 0.0121, f1_measure: 0.0229, loss: 1.6002 ||: 100%|██████████| 35/35 [00:00<00:00, 159.74it/s]\n",
            "accuracy: 0.3674, precision: 0.4558, recall: 0.3362, f1_measure: 0.3870, loss: 1.4455 ||: 100%|██████████| 267/267 [00:09<00:00, 26.86it/s]\n",
            "accuracy: 0.3333, precision: 0.2941, recall: 0.3333, f1_measure: 0.3125, loss: 1.5240 ||: 100%|██████████| 35/35 [00:00<00:00, 175.44it/s]\n",
            "accuracy: 0.4408, precision: 0.4471, recall: 0.6630, f1_measure: 0.5341, loss: 1.3173 ||: 100%|██████████| 267/267 [00:09<00:00, 27.40it/s]\n",
            "accuracy: 0.3533, precision: 0.3266, recall: 0.3939, f1_measure: 0.3571, loss: 1.4736 ||: 100%|██████████| 35/35 [00:00<00:00, 162.61it/s]\n",
            "accuracy: 0.5133, precision: 0.5325, recall: 0.7376, f1_measure: 0.6185, loss: 1.1843 ||: 100%|██████████| 267/267 [00:09<00:00, 27.67it/s]\n",
            "accuracy: 0.3824, precision: 0.3865, recall: 0.3818, f1_measure: 0.3841, loss: 1.5247 ||: 100%|██████████| 35/35 [00:00<00:00, 159.10it/s]\n",
            "accuracy: 0.5772, precision: 0.6566, recall: 0.7616, f1_measure: 0.7052, loss: 1.0509 ||: 100%|██████████| 267/267 [00:09<00:00, 27.72it/s]\n",
            "accuracy: 0.3588, precision: 0.3800, recall: 0.4606, f1_measure: 0.4164, loss: 1.5073 ||: 100%|██████████| 35/35 [00:00<00:00, 158.03it/s]\n",
            "accuracy: 0.6229, precision: 0.7326, recall: 0.7911, f1_measure: 0.7607, loss: 0.9440 ||: 100%|██████████| 267/267 [00:09<00:00, 27.68it/s]\n",
            "accuracy: 0.3669, precision: 0.4121, recall: 0.4121, f1_measure: 0.4121, loss: 1.5348 ||: 100%|██████████| 35/35 [00:00<00:00, 145.02it/s]\n",
            "accuracy: 0.6516, precision: 0.7750, recall: 0.8075, f1_measure: 0.7909, loss: 0.8607 ||: 100%|██████████| 267/267 [00:09<00:00, 27.43it/s]\n",
            "accuracy: 0.3669, precision: 0.3883, recall: 0.4424, f1_measure: 0.4136, loss: 1.6457 ||: 100%|██████████| 35/35 [00:00<00:00, 175.12it/s]\n",
            "accuracy: 0.6807, precision: 0.7891, recall: 0.8393, f1_measure: 0.8134, loss: 0.7949 ||: 100%|██████████| 267/267 [00:10<00:00, 26.49it/s]\n",
            "accuracy: 0.3651, precision: 0.3824, recall: 0.3152, f1_measure: 0.3455, loss: 1.7507 ||: 100%|██████████| 35/35 [00:00<00:00, 158.75it/s]\n",
            "accuracy: 0.7056, precision: 0.8288, recall: 0.8610, f1_measure: 0.8446, loss: 0.7389 ||: 100%|██████████| 267/267 [00:09<00:00, 26.90it/s]\n",
            "accuracy: 0.3524, precision: 0.3702, recall: 0.4061, f1_measure: 0.3873, loss: 1.9829 ||: 100%|██████████| 35/35 [00:00<00:00, 172.89it/s]\n",
            "accuracy: 0.7328, precision: 0.8344, recall: 0.8804, f1_measure: 0.8568, loss: 0.6866 ||: 100%|██████████| 267/267 [00:09<00:00, 27.75it/s]\n",
            "accuracy: 0.3542, precision: 0.3926, recall: 0.3212, f1_measure: 0.3533, loss: 2.0022 ||: 100%|██████████| 35/35 [00:00<00:00, 161.66it/s]\n",
            "accuracy: 0.7535, precision: 0.8380, recall: 0.8874, f1_measure: 0.8620, loss: 0.6374 ||: 100%|██████████| 267/267 [00:09<00:00, 27.49it/s]\n",
            "accuracy: 0.3588, precision: 0.3846, recall: 0.3636, f1_measure: 0.3738, loss: 2.0712 ||: 100%|██████████| 35/35 [00:00<00:00, 160.25it/s]\n",
            "accuracy: 0.7755, precision: 0.8593, recall: 0.9006, f1_measure: 0.8795, loss: 0.5941 ||: 100%|██████████| 267/267 [00:09<00:00, 27.52it/s]\n",
            "accuracy: 0.3470, precision: 0.3602, recall: 0.3515, f1_measure: 0.3558, loss: 2.1974 ||: 100%|██████████| 35/35 [00:00<00:00, 163.20it/s]\n",
            "accuracy: 0.7928, precision: 0.8685, recall: 0.9130, f1_measure: 0.8902, loss: 0.5591 ||: 100%|██████████| 267/267 [00:09<00:00, 27.81it/s]\n",
            "accuracy: 0.3406, precision: 0.3495, recall: 0.3939, f1_measure: 0.3704, loss: 2.3871 ||: 100%|██████████| 35/35 [00:00<00:00, 150.56it/s]\n",
            "accuracy: 0.8076, precision: 0.8760, recall: 0.9161, f1_measure: 0.8956, loss: 0.5195 ||: 100%|██████████| 267/267 [00:09<00:00, 27.07it/s]\n",
            "accuracy: 0.3579, precision: 0.3486, recall: 0.3697, f1_measure: 0.3588, loss: 2.3563 ||: 100%|██████████| 35/35 [00:00<00:00, 140.60it/s]\n",
            "accuracy: 0.8165, precision: 0.8887, recall: 0.9239, f1_measure: 0.9060, loss: 0.4988 ||: 100%|██████████| 267/267 [00:10<00:00, 26.29it/s]\n",
            "accuracy: 0.3433, precision: 0.3351, recall: 0.3818, f1_measure: 0.3569, loss: 2.4595 ||: 100%|██████████| 35/35 [00:00<00:00, 174.42it/s]\n",
            "accuracy: 0.8335, precision: 0.8911, recall: 0.9340, f1_measure: 0.9121, loss: 0.4609 ||: 100%|██████████| 267/267 [00:09<00:00, 26.92it/s]\n",
            "accuracy: 0.3515, precision: 0.3370, recall: 0.3697, f1_measure: 0.3526, loss: 2.7189 ||: 100%|██████████| 35/35 [00:00<00:00, 164.04it/s]\n",
            "accuracy: 0.8449, precision: 0.9058, recall: 0.9402, f1_measure: 0.9227, loss: 0.4344 ||: 100%|██████████| 267/267 [00:09<00:00, 26.72it/s]\n",
            "accuracy: 0.3488, precision: 0.3478, recall: 0.3879, f1_measure: 0.3668, loss: 2.7674 ||: 100%|██████████| 35/35 [00:00<00:00, 174.95it/s]\n",
            "accuracy: 0.8573, precision: 0.9135, recall: 0.9425, f1_measure: 0.9278, loss: 0.4048 ||: 100%|██████████| 267/267 [00:09<00:00, 26.83it/s]\n",
            "accuracy: 0.3442, precision: 0.3351, recall: 0.3818, f1_measure: 0.3569, loss: 2.8745 ||: 100%|██████████| 35/35 [00:00<00:00, 160.14it/s]\n",
            "accuracy: 0.8656, precision: 0.9062, recall: 0.9371, f1_measure: 0.9214, loss: 0.3857 ||: 100%|██████████| 267/267 [00:10<00:00, 26.23it/s]\n",
            "accuracy: 0.3533, precision: 0.3508, recall: 0.4061, f1_measure: 0.3764, loss: 2.8860 ||: 100%|██████████| 35/35 [00:00<00:00, 160.02it/s]\n",
            "accuracy: 0.8707, precision: 0.9153, recall: 0.9480, f1_measure: 0.9314, loss: 0.3722 ||: 100%|██████████| 267/267 [00:10<00:00, 23.73it/s]\n",
            "accuracy: 0.3415, precision: 0.3410, recall: 0.3576, f1_measure: 0.3491, loss: 3.1287 ||: 100%|██████████| 35/35 [00:00<00:00, 176.11it/s]\n",
            "accuracy: 0.8790, precision: 0.9183, recall: 0.9425, f1_measure: 0.9303, loss: 0.3546 ||: 100%|██████████| 267/267 [00:10<00:00, 26.35it/s]\n",
            "accuracy: 0.3633, precision: 0.3536, recall: 0.3879, f1_measure: 0.3699, loss: 3.0808 ||: 100%|██████████| 35/35 [00:00<00:00, 158.23it/s]\n",
            "accuracy: 0.8894, precision: 0.9290, recall: 0.9550, f1_measure: 0.9418, loss: 0.3315 ||: 100%|██████████| 267/267 [00:09<00:00, 26.88it/s]\n",
            "accuracy: 0.3506, precision: 0.3315, recall: 0.3636, f1_measure: 0.3468, loss: 3.2273 ||: 100%|██████████| 35/35 [00:00<00:00, 159.24it/s]\n",
            "accuracy: 0.8997, precision: 0.9356, recall: 0.9589, f1_measure: 0.9471, loss: 0.3065 ||: 100%|██████████| 267/267 [00:10<00:00, 26.66it/s]\n",
            "accuracy: 0.3533, precision: 0.3497, recall: 0.3879, f1_measure: 0.3678, loss: 3.3669 ||: 100%|██████████| 35/35 [00:00<00:00, 162.34it/s]\n",
            "accuracy: 0.8997, precision: 0.9325, recall: 0.9542, f1_measure: 0.9432, loss: 0.2997 ||: 100%|██████████| 267/267 [00:10<00:00, 26.18it/s]\n",
            "accuracy: 0.3542, precision: 0.3434, recall: 0.4121, f1_measure: 0.3747, loss: 3.4011 ||: 100%|██████████| 35/35 [00:00<00:00, 138.47it/s]\n",
            "accuracy: 0.9023, precision: 0.9360, recall: 0.9542, f1_measure: 0.9450, loss: 0.3004 ||: 100%|██████████| 267/267 [00:09<00:00, 26.81it/s]\n",
            "accuracy: 0.3560, precision: 0.3593, recall: 0.3636, f1_measure: 0.3614, loss: 3.4527 ||: 100%|██████████| 35/35 [00:00<00:00, 153.46it/s]\n",
            "accuracy: 0.9078, precision: 0.9376, recall: 0.9565, f1_measure: 0.9470, loss: 0.2821 ||: 100%|██████████| 267/267 [00:09<00:00, 26.83it/s]\n",
            "accuracy: 0.3451, precision: 0.3684, recall: 0.3818, f1_measure: 0.3750, loss: 3.5022 ||: 100%|██████████| 35/35 [00:00<00:00, 160.87it/s]\n",
            "accuracy: 0.9121, precision: 0.9432, recall: 0.9666, f1_measure: 0.9548, loss: 0.2667 ||: 100%|██████████| 267/267 [00:09<00:00, 26.85it/s]\n",
            "accuracy: 0.3551, precision: 0.3600, recall: 0.3273, f1_measure: 0.3429, loss: 3.6087 ||: 100%|██████████| 35/35 [00:00<00:00, 151.06it/s]\n",
            "accuracy: 0.9226, precision: 0.9482, recall: 0.9666, f1_measure: 0.9573, loss: 0.2443 ||: 100%|██████████| 267/267 [00:10<00:00, 26.16it/s]\n",
            "accuracy: 0.3615, precision: 0.3793, recall: 0.4000, f1_measure: 0.3894, loss: 3.6868 ||: 100%|██████████| 35/35 [00:00<00:00, 161.92it/s]\n",
            "accuracy: 0.9302, precision: 0.9563, recall: 0.9674, f1_measure: 0.9618, loss: 0.2210 ||: 100%|██████████| 267/267 [00:09<00:00, 29.81it/s]\n",
            "accuracy: 0.3569, precision: 0.3697, recall: 0.3697, f1_measure: 0.3697, loss: 3.8051 ||: 100%|██████████| 35/35 [00:00<00:00, 161.20it/s]\n",
            "accuracy: 0.9356, precision: 0.9594, recall: 0.9728, f1_measure: 0.9661, loss: 0.2094 ||: 100%|██████████| 267/267 [00:09<00:00, 26.81it/s]\n",
            "accuracy: 0.3551, precision: 0.3602, recall: 0.4061, f1_measure: 0.3818, loss: 3.9662 ||: 100%|██████████| 35/35 [00:00<00:00, 177.39it/s]\n",
            "accuracy: 0.9331, precision: 0.9519, recall: 0.9674, f1_measure: 0.9596, loss: 0.2120 ||: 100%|██████████| 267/267 [00:09<00:00, 27.36it/s]\n",
            "accuracy: 0.3606, precision: 0.3675, recall: 0.3697, f1_measure: 0.3686, loss: 4.0199 ||: 100%|██████████| 35/35 [00:00<00:00, 174.51it/s]\n",
            "accuracy: 0.9327, precision: 0.9548, recall: 0.9674, f1_measure: 0.9610, loss: 0.2178 ||: 100%|██████████| 267/267 [00:09<00:00, 27.07it/s]\n",
            "accuracy: 0.3633, precision: 0.3799, recall: 0.4121, f1_measure: 0.3953, loss: 4.0402 ||: 100%|██████████| 35/35 [00:00<00:00, 162.39it/s]\n",
            "accuracy: 0.9434, precision: 0.9617, recall: 0.9736, f1_measure: 0.9676, loss: 0.1875 ||: 100%|██████████| 267/267 [00:10<00:00, 24.74it/s]\n",
            "accuracy: 0.3542, precision: 0.3646, recall: 0.4000, f1_measure: 0.3815, loss: 4.0863 ||: 100%|██████████| 35/35 [00:00<00:00, 174.27it/s]\n",
            "accuracy: 0.9460, precision: 0.9654, recall: 0.9752, f1_measure: 0.9703, loss: 0.1800 ||: 100%|██████████| 267/267 [00:10<00:00, 26.60it/s]\n",
            "accuracy: 0.3588, precision: 0.3687, recall: 0.4000, f1_measure: 0.3837, loss: 4.1307 ||: 100%|██████████| 35/35 [00:00<00:00, 160.40it/s]\n",
            "accuracy: 0.9441, precision: 0.9600, recall: 0.9697, f1_measure: 0.9649, loss: 0.1803 ||: 100%|██████████| 267/267 [00:09<00:00, 27.05it/s]\n",
            "accuracy: 0.3470, precision: 0.3523, recall: 0.4121, f1_measure: 0.3799, loss: 4.2212 ||: 100%|██████████| 35/35 [00:00<00:00, 164.54it/s]\n",
            "accuracy: 0.9462, precision: 0.9585, recall: 0.9689, f1_measure: 0.9637, loss: 0.1741 ||: 100%|██████████| 267/267 [00:09<00:00, 26.94it/s]\n",
            "accuracy: 0.3497, precision: 0.3642, recall: 0.3818, f1_measure: 0.3728, loss: 4.1995 ||: 100%|██████████| 35/35 [00:00<00:00, 155.76it/s]\n",
            "accuracy: 0.9477, precision: 0.9556, recall: 0.9697, f1_measure: 0.9626, loss: 0.1688 ||: 100%|██████████| 267/267 [00:10<00:00, 26.31it/s]\n",
            "accuracy: 0.3633, precision: 0.3793, recall: 0.4000, f1_measure: 0.3894, loss: 4.1276 ||: 100%|██████████| 35/35 [00:00<00:00, 155.42it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'best_epoch': 5,\n",
              " 'best_validation_accuracy': 0.3533151680290645,\n",
              " 'best_validation_f1_measure': 0.357142835855484,\n",
              " 'best_validation_loss': 1.473589335169111,\n",
              " 'best_validation_precision': 0.32663315534591675,\n",
              " 'best_validation_recall': 0.39393940567970276,\n",
              " 'epoch': 39,\n",
              " 'peak_cpu_memory_MB': 574.66,\n",
              " 'peak_gpu_0_memory_MB': 10,\n",
              " 'training_accuracy': 0.9476825842696629,\n",
              " 'training_cpu_memory_MB': 574.66,\n",
              " 'training_duration': '0:06:46.539712',\n",
              " 'training_epochs': 39,\n",
              " 'training_f1_measure': 0.9626204371452332,\n",
              " 'training_gpu_0_memory_MB': 10,\n",
              " 'training_loss': 0.16876722167661126,\n",
              " 'training_precision': 0.9556235671043396,\n",
              " 'training_recall': 0.9697204828262329,\n",
              " 'training_start_epoch': 0,\n",
              " 'validation_accuracy': 0.36330608537693004,\n",
              " 'validation_f1_measure': 0.389380544424057,\n",
              " 'validation_loss': 4.127551351274763,\n",
              " 'validation_precision': 0.37931033968925476,\n",
              " 'validation_recall': 0.4000000059604645}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cmDS-KOQ2gM",
        "colab_type": "text"
      },
      "source": [
        "## Sanity Check\n",
        "\n",
        "The cell below will allow you to enter sample sentences and test the predictions of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oxRELM_-cako",
        "outputId": "3dcfbc8b-96fa-47b9-a4bf-178e15d4678c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predictor = SentenceClassifierPredictor(model, dataset_reader=reader)\n",
        "logits = predictor.predict(\"Don't waste your money!\")['logits']\n",
        "label_id = np.argmax(logits)\n",
        "\n",
        "print(model.vocab.get_token_from_index(label_id, 'labels'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUXPtPiVQ2gT",
        "colab_type": "text"
      },
      "source": [
        "## More Substantive Checks\n",
        "\n",
        "In order to do some more in depth checks how well the model does, and how well it might generalize we can utilize a set of Amazon reviews. \n",
        "\n",
        "http://jmcauley.ucsd.edu/data/amazon/\n",
        "\n",
        "The site above holds a very large of Amazon reviews that can be used for scientific purposes. \n",
        "\n",
        "### Task 1: Choose and Download a Subcategory\n",
        "\n",
        "From the table below, choose a category that you will use for testing. \n",
        "Download the 5 core links that hold the full text, title and rating of a review. \n",
        "\n",
        "\n",
        "<html>\n",
        "\n",
        "<table>\n",
        "<tbody><tr>\n",
        "  <td>Books</td>\n",
        "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Books_10.json.gz\">10-core</a> (4,701,968 reviews)</td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Books_5.json.gz\">5-core</a> (8,898,041 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Books.csv\">ratings only</a> (22,507,155 ratings)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>Electronics</td>\n",
        "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Electronics_10.json.gz\">10-core</a> (347,393 reviews)</td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Electronics_5.json.gz\">5-core</a> (1,689,188 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Electronics.csv\">ratings only</a> (7,824,482 ratings)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>Movies and TV</td>\n",
        "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Movies_and_TV_10.json.gz\">10-core</a> (958,986 reviews)</td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Movies_and_TV_5.json.gz\">5-core</a> (1,697,533 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Movies_and_TV.csv\">ratings only</a> (4,607,047 ratings)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>CDs and Vinyl</td>\n",
        "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_CDs_and_Vinyl_10.json.gz\">10-core</a> (445,412 reviews)</td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_CDs_and_Vinyl_5.json.gz\">5-core</a> (1,097,592 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_CDs_and_Vinyl.csv\">ratings only</a> (3,749,004 ratings)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>Clothing, Shoes and Jewelry</td>\n",
        "  <!-- <td></td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Clothing_Shoes_and_Jewelry_5.json.gz\">5-core</a> (278,677 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Clothing_Shoes_and_Jewelry.csv\">ratings only</a> (5,748,920 ratings)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>Home and Kitchen</td>\n",
        "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Home_and_Kitchen_10.json.gz\">10-core</a> (25,445 reviews)</td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Home_and_Kitchen_5.json.gz\">5-core</a> (551,682 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Home_and_Kitchen.csv\">ratings only</a> (4,253,926 ratings)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>Kindle Store</td>\n",
        "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Kindle_Store_10.json.gz\">10-core</a> (367,478 reviews)</td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Kindle_Store_5.json.gz\">5-core</a> (982,619 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Kindle_Store.csv\">ratings only</a> (3,205,467 ratings)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>Sports and Outdoors</td>\n",
        "  <!-- <td></td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Sports_and_Outdoors_5.json.gz\">5-core</a> (296,337 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Sports_and_Outdoors.csv\">ratings only</a> (3,268,695 ratings)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>Cell Phones and Accessories</td>\n",
        "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Cell_Phones_and_Accessories_10.json.gz\">10-core</a> (1,854 reviews)</td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Cell_Phones_and_Accessories_5.json.gz\">5-core</a> (194,439 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Cell_Phones_and_Accessories.csv\">ratings only</a> (3,447,249 ratings)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>Health and Personal Care</td>\n",
        "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Health_and_Personal_Care_10.json.gz\">10-core</a> (55,076 reviews)</td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Health_and_Personal_Care_5.json.gz\">5-core</a> (346,355 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Health_and_Personal_Care.csv\">ratings only</a> (2,982,326 ratings)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>Toys and Games</td>\n",
        "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Toys_and_Games_10.json.gz\">10-core</a> (18,637 reviews)</td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Toys_and_Games_5.json.gz\">5-core</a> (167,597 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Toys_and_Games.csv\">ratings only</a> (2,252,771 ratings)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>Video Games</td>\n",
        "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Video_Games_10.json.gz\">10-core</a> (52,158 reviews)</td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Video_Games_5.json.gz\">5-core</a> (231,780 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Video_Games.csv\">ratings only</a> (1,324,753 ratings)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>Tools and Home Improvement</td>\n",
        "  <!-- <td></td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Tools_and_Home_Improvement_5.json.gz\">5-core</a> (134,476 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Tools_and_Home_Improvement.csv\">ratings only</a> (1,926,047 ratings)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>Beauty</td>\n",
        "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Beauty_10.json.gz\">10-core</a> (28,798 reviews)</td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Beauty_5.json.gz\">5-core</a> (198,502 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Beauty.csv\">ratings only</a> (2,023,070 ratings)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>Apps for Android</td>\n",
        "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Apps_for_Android_10.json.gz\">10-core</a> (264,050 reviews)</td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Apps_for_Android_5.json.gz\">5-core</a> (752,937 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Apps_for_Android.csv\">ratings only</a> (2,638,172 ratings)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>Office Products</td>\n",
        "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Office_Products_10.json.gz\">10-core</a> (25,374 reviews)</td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Office_Products_5.json.gz\">5-core</a> (53,258 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Office_Products.csv\">ratings only</a> (1,243,186 ratings)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>Pet Supplies</td>\n",
        "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Pet_Supplies_10.json.gz\">10-core</a> (3,152 reviews)</td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Pet_Supplies_5.json.gz\">5-core</a> (157,836 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Pet_Supplies.csv\">ratings only</a> (1,235,316 ratings)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>Automotive</td>\n",
        "  <!-- <td></td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Automotive_5.json.gz\">5-core</a> (20,473 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Automotive.csv\">ratings only</a> (1,373,768 ratings)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>Grocery and Gourmet Food</td>\n",
        "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Grocery_and_Gourmet_Food_10.json.gz\">10-core</a> (37,348 reviews)</td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Grocery_and_Gourmet_Food_5.json.gz\">5-core</a> (151,254 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Grocery_and_Gourmet_Food.csv\">ratings only</a> (1,297,156 ratings)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>Patio, Lawn and Garden</td>\n",
        "  <!-- <td></td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Patio_Lawn_and_Garden_5.json.gz\">5-core</a> (13,272 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Patio_Lawn_and_Garden.csv\">ratings only</a> (993,490 ratings)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>Baby</td>\n",
        "  <!-- <td></td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Baby_5.json.gz\">5-core</a> (160,792 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Baby.csv\">ratings only</a> (915,446 ratings)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>Digital Music</td>\n",
        "  <!-- <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Digital_Music_10.json.gz\">10-core</a> (22,772 reviews)</td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Digital_Music_5.json.gz\">5-core</a> (64,706 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Digital_Music.csv\">ratings only</a> (836,006 ratings)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>Musical Instruments</td>\n",
        "  <!-- <td></td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Musical_Instruments_5.json.gz\">5-core</a> (10,261 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Musical_Instruments.csv\">ratings only</a> (500,176 ratings)</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "  <td>Amazon Instant Video</td>\n",
        "  <!-- <td></td> -->\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Amazon_Instant_Video_5.json.gz\">5-core</a> (37,126 reviews)</td>\n",
        "  <td><a href=\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/ratings_Amazon_Instant_Video.csv\">ratings only</a> (583,933 ratings)</td>\n",
        "</tr>\n",
        "</tbody>\n",
        "    </table>\n",
        "</html>\n",
        "\n",
        "### Task 2\n",
        "\n",
        "Sanity checks with the reviews.\n",
        "\n",
        "The format of the files is as follows:\n",
        "\n",
        "`{\n",
        "\t\"reviewerID\": \"A2ICI6VUC0U5K6\",\n",
        "\t\"asin\": \"B0014JKKGK\",\n",
        "\t\"reviewerName\": \"Jermin Botrous \\\"gigigigi\\\"\",\n",
        "\t\"helpful\": [0, 0],\n",
        "\t\"reviewText\": \"Don't waste your money because elastic goes bad after 2 washes\",\n",
        "\t\"overall\": 1.0,\n",
        "\t\"summary\": \"One Star\",\n",
        "\t\"unixReviewTime\": 1404432000,\n",
        "\t\"reviewTime\": \"07 4, 2014\"\n",
        "}`\n",
        "\n",
        "Use the following code snippets to load individal review texts.\n",
        "\n",
        "Opening a file in python:\n",
        "\n",
        "``\n",
        "test_file = open('file_name.json', 'r')\n",
        "first_line = test_file.readline()\n",
        "``\n",
        "\n",
        "Transform the line into a json object to access the individual fiels (such as reviewText).\n",
        "\n",
        "\n",
        "``\n",
        "import json\n",
        "j_obj = json.loads(first_line)\n",
        "print('reviewText:' + j_obj['reviewText'])\n",
        "``\n",
        "\n",
        "finally use the code from above to test the predictions\n",
        "\n",
        "``\n",
        "logits = predictor.predict(\"Don't waste your money\")['logits']\n",
        "label_id = np.argmax(logits)\n",
        "prediction = model.vocab.get_token_from_index(label_id, 'labels')\n",
        "print(prediction)\n",
        "``"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gfU2WaMQ2gl",
        "colab_type": "text"
      },
      "source": [
        "## Task 3\n",
        "\n",
        "Iterate over the reviews and extract:\n",
        "\n",
        "* 100 positive predictions (i.e. 4)\n",
        "* 100 negative predictions (i.e. 0)\n",
        "\n",
        "Save the sets of positive and negative predictions as plain text files:\n",
        "\n",
        "* categoryName_100_pos.txt\n",
        "* categoryName_100_neg.txt\n",
        "\n",
        "Manually inspect the predictions to identify potential false positives in boths sets.\n",
        "Store a couple of those false positives in the files:\n",
        "\n",
        "* categoryName_100_pos_fp.txt\n",
        "* categoryName_100_neg_fp.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9n3YbxpQ2gn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!wget http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Movies_and_TV_5.json.gz\n",
        "!gunzip /content/realworldnlp/reviews_Movies_and_TV_5.json.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6sUzkoTQ2gc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "test_file = open('/content/realworldnlp/realworldnlp/reviews_Movies_and_TV_5.json', 'r')\n",
        "positivePredictions = []\n",
        "counter = 0\n",
        "predictor = SentenceClassifierPredictor(model, dataset_reader=reader)\n",
        "\n",
        "\n",
        "first_line = test_file.readline()\n",
        "\n",
        "\n",
        "\n",
        "while first_line:\n",
        "  try:      \n",
        "    counter = counter + 1\n",
        "    j_obj = json.loads(first_line) \n",
        "    if str(j_obj['reviewText']) != \"\":\n",
        "      logits = predictor.predict(str(j_obj['reviewText']))['logits']\n",
        "      label_id = np.argmax(logits)\n",
        "      if 2 > int(model.vocab.get_token_from_index(label_id, 'labels')):\n",
        "        if 2 > int(j_obj['overall']):   \n",
        "          positivePredictions.append(first_line)\n",
        "    first_line = test_file.readline()\n",
        "\n",
        "  except IndexError:\n",
        "    print(\"Sum Thing Wong\")\n",
        "    print(first_line)\n",
        "  if counter > 5000:\n",
        "    break\n",
        "\n",
        "test_file.close()\n",
        "\n",
        "f = open(\"resultfile_tpn.txt\", \"a\")\n",
        "f.write(str(positivePredictions))\n",
        "f.close()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBPe75SKQ2gu",
        "colab_type": "text"
      },
      "source": [
        "## Task 4\n",
        "\n",
        "Generate listing of false positives.\n",
        "Analyse the data from Amazon.\n",
        "What would be a way to utilize this data in order to generate larger lists of false positives?\n",
        "Derive a method that will allow you to predict over the full content of the file and create lists of:\n",
        "* True positive 'positive' predictions\n",
        "* False positive 'positive' predictions\n",
        "* True positive 'negative' predictions\n",
        "* False positive 'negative' predictions\n",
        "\n",
        "Save the four sets four your group submission:\n",
        "\n",
        "* categoryName_pos_tp.txt\n",
        "* categoryName_pos_fp.txt\n",
        "* categoryName_neg_tp.txt\n",
        "* categoryName_neg_fp.txt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G67A_CevQ2gw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6o3_iayQ2g0",
        "colab_type": "text"
      },
      "source": [
        "## Task 5 \n",
        "\n",
        "Calculate approximate precision values based on your mapping from task 4.\n",
        "\n",
        "Store the calculations as part of a readme or send the values by e-mail submission. \n",
        "\n",
        "Submit the files from Task 3 and Task 4 as your group submission.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCJkAPOqQ2g2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}